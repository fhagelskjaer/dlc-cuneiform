{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAQ_Dobsm4DK"
      },
      "source": [
        "# Training time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4_tFuq-TbBC"
      },
      "outputs": [],
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "print(tf.test.gpu_device_name())\n",
        "\n",
        "import sys\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import shutil\n",
        "\n",
        "sys.path.append(\"/content/drive/My Drive/COLAB/COLAB_SRC/\")\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime â†’ \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuxVrpYccpde"
      },
      "outputs": [],
      "source": [
        "import tf_util\n",
        "import provider\n",
        "\n",
        "def get_model(point_cloud, input_label, is_training, cat_num, part_num, \\\n",
        "\t\tbatch_size, num_point, weight_decay, bn_decay=None, use_bn=True,\n",
        "\t\t  k = 20, groups = 32, features = 1024, conv_feat = 64):\n",
        "  \"\"\" ConvNet baseline, input is BxNxF point cloud \"\"\"\n",
        "  end_points = {}\n",
        "\n",
        "  batch_size = point_cloud.get_shape()[0].value\n",
        "  num_point = point_cloud.get_shape()[1].value\n",
        "\n",
        "  with tf.variable_scope('pro/edge_2') as sc:\n",
        "      input_image = tf.expand_dims(point_cloud, -1)\n",
        "      adj = tf_util.pairwise_distance(point_cloud[:,:,:])\n",
        "      nn_idx, values = knn_range(adj, k=k, search_range=60)\n",
        "      edge_feature_2 = tf_util.get_edge_feature(input_image, nn_idx=nn_idx, k=k)\n",
        "\n",
        "  out1 = tf_util.conv2d(edge_feature_2, conv_feat, [1,1],\n",
        "                       padding='VALID', stride=[1,1],\n",
        "                       bn=use_bn, is_training=is_training, weight_decay=weight_decay,\n",
        "                       scope='pro/adj_conv1', bn_decay=bn_decay, activation_fn=tf.nn.elu)\n",
        "  \n",
        "  out1 =  tf.contrib.layers.group_norm(out1, groups=groups, scope='pro/gn_1')\n",
        "  \n",
        "  with tf.variable_scope('pro/red_1') as sc:\n",
        "    net_1 = tf.reduce_max(out1, axis=-2, keep_dims=True)\n",
        "\n",
        "  with tf.variable_scope('pro/edge_3') as sc:\n",
        "      values_mean_3 = tf.reduce_mean(values[:,:,-1])\n",
        "\n",
        "      adj = adj + (tf.round(tf.nn.sigmoid(-adj-values_mean_3))*100000000)\n",
        "      \n",
        "      nn_idx, values = knn_range(adj, k=k, search_range=60)\n",
        "      edge_feature_3 = tf_util.get_edge_feature(net_1, nn_idx=nn_idx, k=k)\n",
        "      \n",
        "  out3 = tf_util.conv2d(edge_feature_3, conv_feat, [1,1],\n",
        "                       padding='VALID', stride=[1,1],\n",
        "                       bn=use_bn, is_training=is_training, weight_decay=weight_decay,\n",
        "                       scope='pro/adj_conv3', bn_decay=bn_decay, activation_fn=tf.nn.elu)\n",
        "\n",
        "  out3 =  tf.contrib.layers.group_norm(out3, groups=groups, scope='pro/gn_3')\n",
        "\n",
        "  with tf.variable_scope('pro/red_2') as sc:\n",
        "    net_2 = tf.reduce_max(out3, axis=-2, keep_dims=True)\n",
        "\n",
        "  out7 = tf_util.conv2d(tf.concat([net_1, net_2], axis=-1, name='pro/concat'), 1024, [1, 1], \n",
        "                       padding='VALID', stride=[1,1],\n",
        "                       bn=use_bn, is_training=is_training,\n",
        "                       scope='pro/adj_conv7', bn_decay=bn_decay, activation_fn=tf.nn.elu)\n",
        "                       \n",
        "  out_mean = tf_util.avg_pool2d(out7, [num_point, 1], padding='VALID', scope='pro/meanpool')\n",
        "\n",
        "  expand = tf.tile(out_mean, [1, num_point, 1, 1], name='pro/expand')\n",
        "  out7_max = tf_util.conv2d(tf.concat([net_1, net_2, expand], axis=-1, name='pro/concat_max'), 1024, [1, 1], \n",
        "                       padding='VALID', stride=[1,1],\n",
        "                       bn=use_bn, is_training=is_training,\n",
        "                       scope='pro/adj_conv7_max', bn_decay=bn_decay, activation_fn=tf.nn.elu)\n",
        "  out_max = tf_util.max_pool2d(out7_max, [num_point, 1], padding='VALID', scope='pro/maxpool')\n",
        "\n",
        "  pool_concat = tf.concat([out_mean, out_max], axis=-1, name='pro/pool_concat')\n",
        "\n",
        "  # MLP on global point cloud vector\n",
        "  net = tf.reshape(pool_concat, [batch_size, -1], name='cla/res')\n",
        "  \n",
        "  net = tf_util.fully_connected(net, 512, bn=use_bn, is_training=is_training,\n",
        "                                scope='cla/fc1', bn_decay=bn_decay, activation_fn=tf.nn.elu)\n",
        "\n",
        "  net = tf_util.dropout(net, keep_prob=0.5, is_training=is_training,\n",
        "                         scope='cla/dp1')\n",
        "                         \n",
        "  net = tf_util.fully_connected(net, 256, bn=use_bn, is_training=is_training,\n",
        "                                scope='cla/fc2', bn_decay=bn_decay, activation_fn=tf.nn.elu)\n",
        "\n",
        "  net = tf_util.dropout(net, keep_prob=0.5, is_training=is_training,\n",
        "                        scope='cla/dp2')\n",
        "  net = tf_util.fully_connected(net, cat_num, activation_fn=None, scope='cla/fc3')\n",
        "\n",
        "  return net, end_points\n",
        "\n",
        "#########################################\n",
        "\n",
        "\n",
        "def get_loss(l_pred, label, end_points):\n",
        "    \n",
        "    per_instance_label_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=l_pred, labels=label)\n",
        "    label_loss = tf.reduce_mean(per_instance_label_loss)\n",
        "\n",
        "    total_loss = label_loss\n",
        "\n",
        "    return total_loss, label_loss, per_instance_label_loss\n",
        "\n",
        "\n",
        "def get_loss_weights(l_pred, label, end_points, weights):\n",
        "    per_instance_label_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=l_pred, labels=label)\n",
        "    label_loss = tf.reduce_mean(tf.multiply(per_instance_label_loss,weights))\n",
        "    total_loss = label_loss\n",
        "    return total_loss, label_loss, per_instance_label_loss\n",
        "\n",
        "def knn_range(adj_matrix, k=20, search_range=20):\n",
        "  \"\"\"Get KNN based on the pairwise distance.\n",
        "  Args:\n",
        "    pairwise distance: (batch_size, num_points, num_points)\n",
        "    k: int\n",
        "    search_range: int\n",
        "\n",
        "  Returns:\n",
        "    nearest neighbors: (batch_size, num_points, k)\n",
        "  \"\"\"\n",
        "  neg_adj = -adj_matrix\n",
        "  values, nn_idx = tf.nn.top_k(neg_adj, k=search_range)\n",
        "  nn_idx = tf.transpose(nn_idx)\n",
        "  nn_idx = tf.random.shuffle(nn_idx)\n",
        "  nn_idx = tf.transpose(nn_idx)\n",
        "  return nn_idx[:,:,:k], values\n",
        "\n",
        "def pairwise_distance(point_cloud):\n",
        "  \"\"\"Compute pairwise distance of a point cloud.\n",
        "\n",
        "  Args:\n",
        "    point_cloud: tensor (batch_size, num_points, num_dims)\n",
        "\n",
        "  Returns:\n",
        "    pairwise distance: (batch_size, num_points, num_points)\n",
        "  \"\"\"\n",
        "  og_batch_size = point_cloud.get_shape().as_list()[0]\n",
        "  point_cloud = tf.squeeze(point_cloud)\n",
        "  if og_batch_size == 1:\n",
        "    point_cloud = tf.expand_dims(point_cloud, 0)\n",
        "    \n",
        "  point_cloud_transpose = tf.transpose(point_cloud, perm=[0, 2, 1])\n",
        "  point_cloud_inner = tf.matmul(point_cloud, point_cloud_transpose)\n",
        "  point_cloud_inner = -2*point_cloud_inner\n",
        "  point_cloud_square = tf.reduce_sum(tf.square(point_cloud), axis=-1, keep_dims=True)\n",
        "  point_cloud_square_tranpose = tf.transpose(point_cloud_square, perm=[0, 2, 1])\n",
        "  return point_cloud_square + point_cloud_inner + point_cloud_square_tranpose\n",
        "\n",
        "\n",
        "def knn(adj_matrix, k=20):\n",
        "  \"\"\"Get KNN based on the pairwise distance.\n",
        "  Args:\n",
        "    pairwise distance: (batch_size, num_points, num_points)\n",
        "    k: int\n",
        "\n",
        "  Returns:\n",
        "    nearest neighbors: (batch_size, num_points, k)\n",
        "  \"\"\"\n",
        "  neg_adj = -adj_matrix\n",
        "  _, nn_idx = tf.nn.top_k(neg_adj, k=k)\n",
        "  return nn_idx\n",
        "\n",
        "def get_edge_feature(point_cloud, nn_idx, k=20):\n",
        "  \"\"\"Construct edge feature for each point\n",
        "  Args:\n",
        "    point_cloud: (batch_size, num_points, 1, num_dims)\n",
        "    nn_idx: (batch_size, num_points, k)\n",
        "    k: int\n",
        "\n",
        "  Returns:\n",
        "    edge features: (batch_size, num_points, k, num_dims)\n",
        "  \"\"\"\n",
        "  og_batch_size = point_cloud.get_shape().as_list()[0]\n",
        "  point_cloud = tf.squeeze(point_cloud)\n",
        "  if og_batch_size == 1:\n",
        "    point_cloud = tf.expand_dims(point_cloud, 0)\n",
        "\n",
        "  point_cloud_central = point_cloud\n",
        "\n",
        "  point_cloud_shape = point_cloud.get_shape()\n",
        "  batch_size = point_cloud_shape[0].value\n",
        "  num_points = point_cloud_shape[1].value\n",
        "  num_dims = point_cloud_shape[2].value\n",
        "\n",
        "  idx_ = tf.range(batch_size) * num_points\n",
        "  idx_ = tf.reshape(idx_, [batch_size, 1, 1]) \n",
        "\n",
        "  point_cloud_flat = tf.reshape(point_cloud, [-1, num_dims])\n",
        "  point_cloud_neighbors = tf.gather(point_cloud_flat, nn_idx+idx_)\n",
        "  point_cloud_central = tf.expand_dims(point_cloud_central, axis=-2)\n",
        "\n",
        "  point_cloud_central = tf.tile(point_cloud_central, [1, 1, k, 1])\n",
        "\n",
        "  edge_feature = tf.concat([point_cloud_central, point_cloud_neighbors-point_cloud_central], axis=-1)\n",
        "  return edge_feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dagyBgZi_wmg"
      },
      "outputs": [],
      "source": [
        "def setup():\n",
        "  global input_dir, ROTATE, gpu_num, wd, point_num, batch_size, output_dir, all_obj_cats, hdf5_data_dir, hdf5_data_dir_train, color_map, NUM_PART_CATS, NUM_CATEGORIES, TRAINING_FILE_LIST, TESTING_FILE_LIST, TRAINING_EPOCHES, MODEL_STORAGE_PATH, LOG_STORAGE_PATH, SUMMARIES_FOLDER\n",
        "  \n",
        "  input_dir = FLAGS_input_dir\n",
        " \n",
        "  ROTATE = bool(FLAGS_rotate)\n",
        " \n",
        "  gpu_num = FLAGS_gpu\n",
        " \n",
        "  wd = FLAGS_wd\n",
        "\n",
        "  point_num = FLAGS_point_num\n",
        "  batch_size = 1\n",
        "  output_dir = FLAGS_output_dir\n",
        " \n",
        "  hdf5_data_dir = os.path.join(BASE_DIR, './' + input_dir)\n",
        " \n",
        "  hdf5_data_dir_train = ''\n",
        " \n",
        "  if not os.path.exists(output_dir):\n",
        "      os.mkdir(output_dir)\n",
        " \n",
        "  color_map_file = os.path.join(hdf5_data_dir, 'part_color_mapping.json')\n",
        "  color_map = json.load(open(color_map_file, 'r'))\n",
        " \n",
        "  all_obj_cats_file = os.path.join(hdf5_data_dir, 'all_object_categories.txt')\n",
        "  fin = open(all_obj_cats_file, 'r')\n",
        "  lines = [line.rstrip() for line in fin.readlines()]\n",
        "  all_obj_cats = [(line.split()[0], line.split()[1]) for line in lines]\n",
        "  fin.close()\n",
        " \n",
        "  all_cats = json.load(open(os.path.join(hdf5_data_dir, 'overallid_to_catid_partid.json'), 'r'))\n",
        " \n",
        "  print( \"all_obj_cats_file\", all_obj_cats )\n",
        " \n",
        "  NUM_CATEGORIES = len(all_obj_cats)\n",
        "  NUM_PART_CATS = 1\n",
        " \n",
        "  print('#### Batch Size: {0}'.format(batch_size))\n",
        "  print('#### Point Number: {0}'.format(point_num))\n",
        "  print('#### Training using GPU: {0}'.format(gpu_num))\n",
        " \n",
        " \n",
        "  TRAINING_EPOCHES = FLAGS_epoch\n",
        " \n",
        "  print('### Training epoch: {0}'.format(TRAINING_EPOCHES))\n",
        " \n",
        "  TRAINING_FILE_LIST = os.path.join(hdf5_data_dir, 'train_hdf5_file_list.txt')\n",
        "  TESTING_FILE_LIST = os.path.join(hdf5_data_dir, 'val_hdf5_file_list.txt')\n",
        " \n",
        "  MODEL_STORAGE_PATH = os.path.join(output_dir, 'trained_models_o3d')\n",
        "  if not os.path.exists(MODEL_STORAGE_PATH):\n",
        "      os.mkdir(MODEL_STORAGE_PATH)\n",
        " \n",
        "  LOG_STORAGE_PATH = os.path.join(output_dir, 'logs')\n",
        "  if not os.path.exists(LOG_STORAGE_PATH):\n",
        "      os.mkdir(LOG_STORAGE_PATH)\n",
        " \n",
        "  SUMMARIES_FOLDER =  os.path.join(output_dir, 'summaries')\n",
        "  if not os.path.exists(SUMMARIES_FOLDER):\n",
        "      os.mkdir(SUMMARIES_FOLDER) \n",
        " \n",
        "def printout(flog, data):\n",
        "    print(data)\n",
        "    flog.write(data + '\\n')\n",
        " \n",
        "def placeholder_inputs():\n",
        "    pointclouds_ph = tf.placeholder(tf.float32, shape=(batch_size, point_num, 6))\n",
        "    input_label_ph = tf.placeholder(tf.float32, shape=(batch_size, NUM_CATEGORIES))\n",
        "    labels_ph = tf.placeholder(tf.int32, shape=(batch_size))\n",
        "    return pointclouds_ph, input_label_ph, labels_ph\n",
        " \n",
        " \n",
        "def convert_label_to_one_hot(labels):\n",
        "    label_one_hot = np.zeros((labels.shape[0], NUM_CATEGORIES))\n",
        "    for idx in range(labels.shape[0]):\n",
        "        label_one_hot[idx, labels[idx]] = 1\n",
        "    return label_one_hot\n",
        " \n",
        "def get_weights_inverse_num_of_samples(no_of_classes, samples_per_cls, power=1):\n",
        "    weights_for_samples = 1.0 / np.array(np.power(samples_per_cls, power))\n",
        "    weights_for_samples = weights_for_samples / np.sum(weights_for_samples) * no_of_classes\n",
        "    return weights_for_samples\n",
        " \n",
        "def train():\n",
        "  with tf.Graph().as_default():\n",
        "        with tf.device('/gpu:'+str(gpu_num)):\n",
        "            pointclouds_ph, input_label_ph, labels_ph = placeholder_inputs()\n",
        "            is_training_ph = tf.placeholder(tf.bool, shape=())\n",
        " \n",
        "            min_batch_size = 1\n",
        " \n",
        "            batch = tf.Variable(0, trainable=False)\n",
        "            learning_rate = tf.train.exponential_decay(\n",
        "                            BASE_LEARNING_RATE,     # base learning rate\n",
        "                            batch * min_batch_size,     # global_var indicating the number of steps\n",
        "                            DECAY_STEP,             # step size\n",
        "                            DECAY_RATE,             # decay rate\n",
        "                            staircase=True          # Stair-case or continuous decreasing\n",
        "                            )\n",
        "            learning_rate = tf.maximum(learning_rate, LEARNING_RATE_CLIP)\n",
        "        \n",
        "            bn_momentum = tf.train.exponential_decay(\n",
        "                      BN_INIT_DECAY,\n",
        "                      batch*min_batch_size,\n",
        "                      BN_DECAY_DECAY_STEP,\n",
        "                      BN_DECAY_DECAY_RATE,\n",
        "                      staircase=True)\n",
        "            bn_decay = tf.minimum(BN_DECAY_CLIP, 1 - bn_momentum)\n",
        " \n",
        "            lr_op = tf.summary.scalar('learning_rate', learning_rate)\n",
        "            batch_op = tf.summary.scalar('batch_number', batch)\n",
        "            bn_decay_op = tf.summary.scalar('bn_decay', bn_decay)\n",
        " \n",
        "            with tf.variable_scope(NAMING_SCOPE): \n",
        "              labels_pred, end_points = get_model(pointclouds_ph, input_label_ph, \\\n",
        "                      is_training=is_training_ph, bn_decay=bn_decay, cat_num=NUM_CATEGORIES, \\\n",
        "                      part_num=NUM_PART_CATS, batch_size=1, num_point=point_num, weight_decay=wd, use_bn=False)\n",
        " \n",
        "            weights_ph = tf.placeholder(tf.float32, shape=(1))\n",
        " \n",
        "            loss, label_loss, per_instance_label_loss, \\\n",
        "                = get_loss_weights(labels_pred, labels_ph, end_points, weights_ph)\n",
        " \n",
        "            total_training_loss_ph = tf.placeholder(tf.float32, shape=())\n",
        "            total_testing_loss_ph = tf.placeholder(tf.float32, shape=())\n",
        " \n",
        "            label_training_loss_ph = tf.placeholder(tf.float32, shape=())\n",
        "            label_testing_loss_ph = tf.placeholder(tf.float32, shape=())\n",
        " \n",
        " \n",
        "            label_training_acc_ph = tf.placeholder(tf.float32, shape=())\n",
        "            label_testing_acc_ph = tf.placeholder(tf.float32, shape=())\n",
        " \n",
        "            total_train_loss_sum_op = tf.summary.scalar('total_training_loss', total_training_loss_ph)\n",
        "            total_test_loss_sum_op = tf.summary.scalar('total_testing_loss', total_testing_loss_ph)\n",
        " \n",
        "            label_train_loss_sum_op = tf.summary.scalar('label_training_loss', label_training_loss_ph)\n",
        "            label_test_loss_sum_op = tf.summary.scalar('label_testing_loss', label_testing_loss_ph)\n",
        " \n",
        " \n",
        "            label_train_acc_sum_op = tf.summary.scalar('label_training_acc', label_training_acc_ph)\n",
        "            label_test_acc_sum_op = tf.summary.scalar('label_testing_acc', label_testing_acc_ph)\n",
        " \n",
        "            train_variables = tf.trainable_variables()\n",
        " \n",
        "            trainer = tf.train.AdamOptimizer(learning_rate)\n",
        "            train_op = trainer.minimize(loss, var_list=train_variables, global_step=batch)\n",
        " \n",
        "            # 0) Retrieve trainable variables\n",
        "            tvs = tf.trainable_variables() \n",
        "            # 1) Create placeholders for the accumulating gradients we'll be storing\n",
        "            accum_vars = [tf.Variable(tv.initialized_value(),\n",
        "                          trainable=False) for tv in tvs]\n",
        "            # 2) Operation to initialize accum_vars to zero\n",
        "            zero_ops  = [tv.assign(tf.zeros_like(tv)) for tv in accum_vars]\n",
        "            # 3) Operation to compute the gradients for one minibatch\n",
        "            gvs = trainer.compute_gradients(loss)\n",
        "            # 4) Operation to accumulate the gradients in accum_vars\n",
        "            accum_ops = [accum_vars[i].assign_add(gv[0]) for i, gv in enumerate(gvs)]\n",
        "            # 5) Operation to perform the update (apply gradients)\n",
        "            apply_ops = trainer.apply_gradients([(accum_vars[i], tv) for i, tv in enumerate(tf.trainable_variables())], global_step=batch)\n",
        "            \n",
        "            \n",
        "        saver = tf.train.Saver(max_to_keep=5)\n",
        " \n",
        "        config = tf.ConfigProto()\n",
        "        config.gpu_options.allow_growth = False\n",
        "        config.allow_soft_placement = True\n",
        "        sess = tf.Session(config=config)\n",
        "      \n",
        "        init = tf.global_variables_initializer()\n",
        "        sess.run(init)\n",
        " \n",
        "        train_writer = tf.summary.FileWriter(SUMMARIES_FOLDER + '/train', sess.graph)\n",
        "        test_writer = tf.summary.FileWriter(SUMMARIES_FOLDER + '/test')\n",
        " \n",
        "        train_file_list = provider.getDataFiles(TRAINING_FILE_LIST)\n",
        "        num_train_file = len(train_file_list)\n",
        "        test_file_list = provider.getDataFiles(TESTING_FILE_LIST)\n",
        "        num_test_file = len(test_file_list)\n",
        "\n",
        "        # write logs to the disk\n",
        "        if( STORED_MODEL_PATH != '' ):\n",
        "          flog = open(os.path.join(LOG_STORAGE_PATH, 'log.txt'), 'a')\n",
        "        else:\n",
        "          flog = open(os.path.join(LOG_STORAGE_PATH, 'log.txt'), 'w')\n",
        "\n",
        "        def train_one_epoch(train_file_idx, epoch_num):\n",
        "            is_training = True\n",
        " \n",
        "            for i in range(num_train_file):\n",
        "                cur_train_filename = os.path.join(hdf5_data_dir_train, train_file_list[train_file_idx[i]])\n",
        "                printout(flog, 'Loading train file ' + cur_train_filename)\n",
        " \n",
        "                cur_data, cur_labels = provider.loadDataFile(cur_train_filename)\n",
        " \n",
        "                print( cur_data.shape, cur_labels.shape )\n",
        " \n",
        "\n",
        "                cur_labels = np.squeeze(cur_labels)\n",
        "\n",
        "\n",
        "                cur_data, cur_labels, order = provider.shuffle_data(cur_data, cur_labels)\n",
        "                print(cur_data.shape, cur_labels.shape)\n",
        " \n",
        "                weights_for_samples = get_weights_inverse_num_of_samples( NUM_CATEGORIES, [ list(cur_labels).count(i) for i in range(NUM_CATEGORIES)] )\n",
        "                print(\"weigths\", weights_for_samples)\n",
        "\n",
        "                cur_labels_one_hot = convert_label_to_one_hot(cur_labels)\n",
        "\n",
        "                num_data = len(cur_labels)\n",
        "                num_batch = num_data // min_batch_size\n",
        " \n",
        "                total_loss = 0.0\n",
        "                total_label_loss = 0.0\n",
        "                total_label_acc = 0.0\n",
        " \n",
        "                total_guess_class = [0 for _ in range(NUM_CATEGORIES)]\n",
        "                total_seen_class = [0 for _ in range(NUM_CATEGORIES)]\n",
        "                total_correct_class = [0 for _ in range(NUM_CATEGORIES)]\n",
        " \n",
        "                print( cur_data.shape )\n",
        " \n",
        "                for j in range(num_batch):\n",
        " \n",
        "                    begidx = j * min_batch_size\n",
        "                    endidx = (j + 1) * min_batch_size\n",
        "                    \n",
        "                    train_data = provider.shuffle_point_cloud(cur_data[begidx: endidx, ...])\n",
        "\n",
        " \n",
        "                    sess.run(zero_ops)\n",
        "                    for data_min_batch, min_batch in enumerate(range(begidx,endidx)):\n",
        "                      cur_weights = []\n",
        "                      for label_itr in cur_labels[min_batch: min_batch+1, ...]:\n",
        "                        cur_weights.append(weights_for_samples[label_itr])\n",
        "                      cur_weights = np.array(cur_weights,np.float32)\n",
        "\n",
        " \n",
        "\n",
        "                      feed_dict = {\n",
        "                              pointclouds_ph: train_data[data_min_batch: data_min_batch+1, :point_num, :],\n",
        "                              labels_ph: cur_labels[min_batch: min_batch+1, ...], \n",
        "                              input_label_ph: cur_labels_one_hot[min_batch: min_batch+1, ...],\n",
        "                              is_training_ph: is_training, \n",
        "                              weights_ph: cur_weights,\n",
        "                              }\n",
        " \n",
        "                      _, loss_val, label_loss_val, per_instance_label_loss_val, label_pred_val = \\\n",
        "                        sess.run([accum_ops, loss, label_loss, per_instance_label_loss, labels_pred], feed_dict=feed_dict)\n",
        "\n",
        " \n",
        "                      if( np.isnan(loss_val) ):\n",
        "                        import pdb; pdb.set_trace()\n",
        " \n",
        "                      total_loss += loss_val\n",
        "                      total_label_loss += label_loss_val\n",
        " \n",
        "                      per_instance_label_pred = np.argmax(label_pred_val, axis=1)\n",
        "                      total_label_acc += np.mean(np.float32(per_instance_label_pred == cur_labels[min_batch: min_batch+1, ...]))\n",
        " \n",
        " \n",
        "                      for i in range(min_batch, min_batch+1):\n",
        "                        l = cur_labels[i]\n",
        "                        total_guess_class[ per_instance_label_pred[i-min_batch] ] += 1\n",
        "                        total_seen_class[l] += 1\n",
        "                        total_correct_class[l] += (per_instance_label_pred[i-min_batch] == l)\n",
        " \n",
        "                    sess.run(apply_ops)\n",
        " \n",
        "                total_loss = total_loss * 1.0 / num_data\n",
        "                \n",
        "                total_label_loss = total_label_loss * 1.0 / num_data\n",
        "                \n",
        "                total_label_acc = total_label_acc * 1.0 / num_data\n",
        " \n",
        "                lr_sum, bn_decay_sum, batch_sum, train_loss_sum, train_label_acc_sum, \\\n",
        "                        train_label_loss_sum, \\\n",
        "                        = \\\n",
        "                        sess.run(\\\n",
        "                        [lr_op, bn_decay_op, batch_op, total_train_loss_sum_op, label_train_acc_sum_op, \\\n",
        "                        label_train_loss_sum_op ], \\\n",
        "                        feed_dict={total_training_loss_ph: total_loss, label_training_loss_ph: total_label_loss, \\\n",
        "                        label_training_acc_ph: total_label_acc, \\\n",
        "                        })\n",
        " \n",
        "                train_writer.add_summary(train_loss_sum, i + epoch_num * num_train_file)\n",
        "                train_writer.add_summary(train_label_loss_sum, i + epoch_num * num_train_file)\n",
        "             \n",
        "                \n",
        "                train_writer.add_summary(lr_sum, i + epoch_num * num_train_file)\n",
        "                train_writer.add_summary(bn_decay_sum, i + epoch_num * num_train_file)\n",
        "                train_writer.add_summary(train_label_acc_sum, i + epoch_num * num_train_file)\n",
        "                \n",
        "                train_writer.add_summary(batch_sum, i + epoch_num * num_train_file)\n",
        " \n",
        "                learning_rate_show, batch_show, bn_decay_show = sess.run([learning_rate, batch, bn_decay])\n",
        " \n",
        "                printout(flog, '\\tLearning Rate: %f' % learning_rate_show)\n",
        "                printout(flog, '\\tbn_decay: %f' % bn_decay_show)\n",
        "                printout(flog, '\\tbatch: %f' % batch_show)\n",
        " \n",
        "                printout(flog, '\\tTraining Total Mean_loss: %f' % total_loss)\n",
        "                printout(flog, '\\t\\tTraining Label Mean_loss: %f' % total_label_loss)\n",
        "                printout(flog, '\\t\\tTraining Label Accuracy: %f' % total_label_acc)\n",
        "        \n",
        "                print( \"Per instance: \")\n",
        "                for k in range(NUM_CATEGORIES):\n",
        "                  print( '\\t' + all_obj_cats[k][0], 'XX' if total_guess_class[k] == 0 else total_correct_class[k]/total_guess_class[k], total_correct_class[k]/total_seen_class[k], '---', total_correct_class[k], total_seen_class[k], total_guess_class[k])\n",
        " \n",
        " \n",
        "        def eval_one_epoch(epoch_num):\n",
        "            is_training = False\n",
        " \n",
        "            total_loss = 0.0\n",
        "            total_label_loss = 0.0\n",
        "            total_label_acc = 0.0\n",
        " \n",
        "            total_guess_class = [0 for _ in range(NUM_CATEGORIES)]\n",
        "            total_seen_class = [0 for _ in range(NUM_CATEGORIES)]\n",
        "            total_correct_class = [0 for _ in range(NUM_CATEGORIES)]\n",
        " \n",
        "            for i in range(num_test_file):\n",
        "                cur_test_filename = os.path.join(hdf5_data_dir_train, test_file_list[i])\n",
        "                printout(flog, 'Loading test file ' + cur_test_filename)\n",
        " \n",
        "                print( \"cur\", cur_test_filename )\n",
        " \n",
        "                cur_data, cur_labels = provider.loadDataFile(cur_test_filename)\n",
        "                cur_labels = np.squeeze(cur_labels)\n",
        "\n",
        "                weights_for_samples = get_weights_inverse_num_of_samples( NUM_CATEGORIES, [ list(cur_labels).count(i) for i in range(NUM_CATEGORIES)] )\n",
        "                print(\"weigths\", weights_for_samples)\n",
        "\n",
        "                cur_labels_one_hot = convert_label_to_one_hot(cur_labels)\n",
        " \n",
        "                num_data = len(cur_labels)\n",
        "                num_batch = num_data // batch_size\n",
        " \n",
        "                for j in range(num_batch):\n",
        "                    begidx = j * batch_size\n",
        "                    endidx = (j + 1) * batch_size\n",
        " \n",
        "                    cur_weights = []\n",
        "                    for label_itr in cur_labels[begidx: endidx, ...]:\n",
        "                      cur_weights.append(weights_for_samples[label_itr])\n",
        "                    cur_weights = np.array(cur_weights,np.float32)\n",
        " \n",
        " \n",
        "                    feed_dict = {\n",
        "                            pointclouds_ph: cur_data[begidx: endidx, :point_num,:],\n",
        "                            labels_ph: cur_labels[begidx: endidx, ...], \n",
        "                            input_label_ph: cur_labels_one_hot[begidx: endidx, ...], \n",
        "                            is_training_ph: is_training, \n",
        "                            weights_ph: cur_weights,\n",
        "                            }\n",
        " \n",
        "                    loss_val, label_loss_val, per_instance_label_loss_val, label_pred_val,\\\n",
        "                            = sess.run([loss, label_loss, per_instance_label_loss, \\\n",
        "                            labels_pred ], \\\n",
        "                            feed_dict=feed_dict)\n",
        "\n",
        "                    total_loss += loss_val\n",
        "                    total_label_loss += label_loss_val\n",
        " \n",
        "                    per_instance_label_pred = np.argmax(label_pred_val, axis=1)\n",
        "                    total_label_acc += np.mean(np.float32(per_instance_label_pred == cur_labels[begidx: endidx, ...]))\n",
        "                    \n",
        "                    for i in range(begidx, endidx):\n",
        "                      total_guess_class[ per_instance_label_pred[i-begidx] ] += 1\n",
        "                      l = cur_labels[i]\n",
        "                      total_seen_class[l] += 1\n",
        "                      total_correct_class[l] += (per_instance_label_pred[i-begidx] == l)\n",
        " \n",
        "            total_loss = total_loss * 1.0 / num_batch            \n",
        "            total_label_loss = total_label_loss * 1.0 / num_batch            \n",
        "            total_label_acc = total_label_acc * 1.0 / num_batch\n",
        " \n",
        "            test_loss_sum, test_label_acc_sum, \\\n",
        "                    test_label_loss_sum, \\\n",
        "                    = \\\n",
        "                    sess.run(\\\n",
        "                    [ total_test_loss_sum_op, label_test_acc_sum_op, label_test_loss_sum_op ], \\\n",
        "                    feed_dict={total_testing_loss_ph: total_loss, label_testing_loss_ph: total_label_loss, label_testing_acc_ph: total_label_acc})\n",
        " \n",
        "            test_writer.add_summary(test_loss_sum, i + epoch_num * num_test_file)\n",
        "            test_writer.add_summary(test_label_loss_sum, i + epoch_num * num_test_file)\n",
        "            test_writer.add_summary(test_label_acc_sum, i + epoch_num * num_test_file)\n",
        " \n",
        "            printout(flog, '\\tTest Total Mean_loss: %f' % total_loss)\n",
        "            printout(flog, '\\t\\tTest Label Mean_loss: %f' % total_label_loss)\n",
        "            printout(flog, '\\t\\tTest Label Accuracy: %f' % total_label_acc)\n",
        " \n",
        "            mean_pre = 0\n",
        "            mean_rec = 0\n",
        " \n",
        "            print( \"\\tPer instance: \")\n",
        "            for k in range(NUM_CATEGORIES):\n",
        "                  print( '\\t' + all_obj_cats[k][0], 'XX' if total_guess_class[k] == 0 else total_correct_class[k]/total_guess_class[k], total_correct_class[k]/total_seen_class[k], '---', total_correct_class[k], total_seen_class[k], total_guess_class[k])\n",
        "                  mean_pre += (0 if total_guess_class[k] == 0 else total_correct_class[k]/total_guess_class[k])*total_seen_class[k]\n",
        "                  mean_rec += (total_correct_class[k]/total_seen_class[k])*total_seen_class[k]\n",
        "            precision = mean_pre/np.sum(total_seen_class)\n",
        "            recall = mean_rec/np.sum(total_seen_class)\n",
        "            print( '\\t\\tAvg', mean_pre/np.sum(total_seen_class), mean_rec/np.sum(total_seen_class), 2*((precision*recall)/(precision+recall)) )\n",
        "            print('*****************************')\n",
        " \n",
        "        if not os.path.exists(MODEL_STORAGE_PATH):\n",
        "            os.mkdir(MODEL_STORAGE_PATH)\n",
        " \n",
        "        for epoch in range(1, TRAINING_EPOCHES + 1):\n",
        " \n",
        "            printout(flog, '\\n>>> Training for the epoch %d/%d ...' % (epoch, TRAINING_EPOCHES))\n",
        " \n",
        "            train_file_idx = np.arange(0, len(train_file_list))\n",
        "            np.random.shuffle(train_file_idx)\n",
        " \n",
        "            seconds = time.time()\n",
        " \n",
        "            train_one_epoch(train_file_idx, epoch)\n",
        " \n",
        "            print('Epoch took %0.3f s' % (time.time()-seconds) )\n",
        " \n",
        "            if( (epoch) % 5 == 0 ):\n",
        "                printout(flog, '\\n<<< Testing on the test dataset ...')\n",
        "                eval_one_epoch(epoch)\n",
        " \n",
        "            if( (epoch) % 100 == 0 ):\n",
        "                cp_filename = saver.save(sess, os.path.join(MODEL_STORAGE_PATH, 'epoch_' + str(epoch)+'.ckpt'))\n",
        "                printout(flog, 'Successfully store the checkpoint model into ' + cp_filename)\n",
        " \n",
        "            flog.flush()\n",
        " \n",
        "        flog.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjepAXtb9p8F"
      },
      "outputs": [],
      "source": [
        "FLAGS_input_dir = ''\n",
        "FLAGS_rotate = True\n",
        "\n",
        "FLAGS_point_num = 32768\n",
        "\n",
        "FLAGS_batch = 1\n",
        "\n",
        "FLAGS_gpu = 0\n",
        "FLAGS_epoch = 200\n",
        "FLAGS_wd = 0.05\n",
        "\n",
        "NAMING_SCOPE = \"empty\"\n",
        "\n",
        "STORED_MODEL_PATH = \"\" \n",
        "\n",
        "DECAY_STEP = 10000\n",
        "DECAY_RATE = 0.5\n",
        "LEARNING_RATE_CLIP = 1e-7\n",
        "BASE_LEARNING_RATE = 0.0001\n",
        "\n",
        "BN_INIT_DECAY = 0.5\n",
        "BN_DECAY_DECAY_RATE = 0.5\n",
        "BN_DECAY_DECAY_STEP = float(DECAY_STEP * 2)\n",
        "BN_DECAY_CLIP = 0.99"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gS2dvZlqrkPi"
      },
      "outputs": [],
      "source": [
        "# Run left sign precense classification\n",
        "\n",
        "NAMING_SCOPE = 'cuneiform_left'\n",
        "BASE_DIR = '/content/drive/My Drive/COLAB/COLAB_DATA/' + NAMING_SCOPE \n",
        "shutil.copy2(BASE_DIR+\"/cuneiform_file_o3d_left_train.h5\", \".\")\n",
        "shutil.copy2(BASE_DIR+\"/cuneiform_file_o3d_left_test.h5\", \".\")\n",
        "FLAGS_output_dir = BASE_DIR\n",
        "setup()\n",
        "train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZRl1hda-D2DG"
      },
      "outputs": [],
      "source": [
        "# Run seal precense classification\n",
        "\n",
        "NAMING_SCOPE = 'cuneiform_seal'\n",
        "BASE_DIR = '/content/drive/My Drive/COLAB/COLAB_DATA/' + NAMING_SCOPE \n",
        "shutil.copy2(BASE_DIR+\"/cuneiform_file_o3d_seal_train.h5\", \".\")\n",
        "shutil.copy2(BASE_DIR+\"/cuneiform_file_o3d_seal_test.h5\", \".\")\n",
        "FLAGS_output_dir = BASE_DIR\n",
        "setup()\n",
        "train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ph3nn3tTVO8x"
      },
      "outputs": [],
      "source": [
        "# Run time period comparison with identical dataset\n",
        "\n",
        "NAMING_SCOPE = 'cuneiform_time_period'\n",
        "BASE_DIR = '/content/drive/My Drive/COLAB/COLAB_DATA/' + NAMING_SCOPE \n",
        "shutil.copy2(BASE_DIR+\"/cuneiform_file_o3d_train.h5\", \".\")\n",
        "shutil.copy2(BASE_DIR+\"/cuneiform_file_o3d_test.h5\", \".\")\n",
        "FLAGS_output_dir = BASE_DIR\n",
        "setup()\n",
        "train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AT7tNNV7tPuk"
      },
      "outputs": [],
      "source": [
        "# Run time period comparison with large dataset\n",
        "\n",
        "NAMING_SCOPE = 'cuneiform_time_period'\n",
        "BASE_DIR = '/content/drive/My Drive/COLAB/COLAB_DATA/' + NAMING_SCOPE \n",
        "shutil.copy2(BASE_DIR+\"/cuneiform_file_o3d_train_l.h5\", \"cuneiform_file_o3d_train.h5\")\n",
        "shutil.copy2(BASE_DIR+\"/cuneiform_file_o3d_test.h5\", \".\")\n",
        "FLAGS_output_dir = BASE_DIR\n",
        "setup()\n",
        "train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "cuneiform_train_model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}